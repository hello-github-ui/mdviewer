
# 大数据基础

> Hadoop 三大发行版本：Apache、Cloudera、Hortonworks。这里以普通的 Apache Hadoop 方式搭建大数据环境（后面会考虑使用更贴合实际的 CDH 版本的 Hadoop集群）

## Hadoop Yarn集群的搭建

###  虚拟机的准备

> 先准备三台centos7虚拟机，具体方式不做演示

#### 修改 IP

1. 第一台虚拟机修改为如下内容：

```shell
#  vim /etc/sysconfig/network-scripts/ifcfgens33
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=static
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=ens33
UUID=29b35945-26d7-4f4f-9924-ff97b9bf0638
DEVICE=ens33
ONBOOT=yes
IPADDR=10.144.144.102
PREFIX=24
GATEWAY=10.144.144.2
DNS1=10.144.144.2
```

2. 第二台虚拟机修改为如下内容：

```shell
BOOTPROTO="static"
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="ens33"
UUID="07ad7c52-3d68-4e89-a699-8be6a0bd92d0"
DEVICE="ens33"
ONBOOT="yes"
IPADDR="10.144.144.103"
PREFIX="24"
GATEWAY="10.144.144.2"
DNS1="10.144.144.2"
```

2. 第三台虚拟机修改为如下内容：

```shell
BOOTPROTO="static"
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="ens33"
UUID="07ad7c52-3d68-4e89-a699-8be6a0bd92d0"
DEVICE="ens33"
ONBOOT="yes"
IPADDR="10.144.144.104"
PREFIX="24"
GATEWAY="10.144.144.2"
DNS1="10.144.144.2"
```

#### 修改主机名

1. 第一台虚拟机修改为如下内容：

```shell
# vim /etc/hostname
hadoop102
```

1. 第二台虚拟机修改为如下内容：

```shell
# vim /etc/hostname
hadoop103
```
2. 第三台虚拟机修改为如下内容：

```shell
# vim /etc/hostname
hadoop104
```

#### 配置 Linux 克隆机主机名称映射 hosts 文件，打开/etc/hosts

```shell
# vim /etc/hosts 三台主机都如此修改即可
10.144.144.144 my_centos
10.144.144.102 hadoop102
10.144.144.103 hadoop103
10.144.144.104 hadoop104
10.144.144.254 my_portable_centos
```

#### Windows host文件配置

```shell
# windows/system32/drivers/etc/hosts
10.144.144.144 my_centos
10.144.144.102 hadoop102
10.144.144.103 hadoop103
10.144.144.104 hadoop104
10.144.144.254 my_portable_centos
```

#### 卸载自带的 jdk

```shell
rpm -qa | grep -i java | xargs -n1 rpm -e --nodeps
```

➢ rpm -qa：查询所安装的所有 rpm 软件包

➢ grep -i：忽略大小写

➢ xargs -n1：表示每次只传递一个参数

➢ rpm -e –nodeps：强制卸载软件

#### 关闭防火墙并禁用开机自启动防火墙

```shell
# hadoop102 hadoop103 hadoop104 都需要如下配置
systemctl stop firewalld
systemctl disable firewalld.service
```

#### 创建普通用户

```shell
# 使用 root 账户操作
 useradd admin
 passwd admin # 回车后，提示输入新密码，建议123456
```

#### 配置 admin 用户具有 root 权限，方便后期加 sudo 执行 root 权限的命令

修改/etc/sudoers 文件，在%wheel 这行下面添加一行，如下所示：

```shell
# vim /etc/sudoers
## Allow root to run any commands anywhere
root ALL=(ALL) ALL
## Allows people in group wheel to run all commands
%wheel ALL=(ALL) ALL
admin ALL=(ALL) NOPASSWD:ALL # 注意，位置一定要放到 wheel 下面一行
```

>  注意：admin 这一行不要直接放到 root 行下面，因为所有用户都属于 wheel 组，你先 配置了 admin 具有免密功能，但是程序执行到%wheel 行时，该功能又被覆盖回需要 密码。所以 admin 要放到%wheel 这行下面。

#### 在/opt 目录下创建文件夹，并修改所属主和所属组

（1）在/opt 目录下创建 module、software 文件夹 【三台主机都需要如下操作，如果你是采用的clone模式另外两台主机，只需要Hadoop102操作即可】

```shell
[root@hadoop102 ~]# mkdir /opt/module
[root@hadoop102 ~]# mkdir /opt/software
```

（2）修改 module、software 文件夹的所有者和所属组均为 admin 用户

```shell
[root@hadoop102 ~]# chown admin:admin /opt/module
[root@hadoop102 ~]# chown admin:admin /opt/software
```

> 三台主机都编辑完成后，reboot 。

#### Hadoop102上安装jdk

* 用 XShell 传输工具将 JDK 导入到 opt 目录下面的 software 文件夹下面

* 解压 JDK 到/opt/module 目录下 `tar -zxvf jdkjdk-8u311-linux-x64.tar.gz -C /opt/module/`

* 配置 JDK 环境变量

  （1）新建/etc/profile.d/my_env.sh 文件

  ```shell
  [admin@hadoop102 ~]$ sudo vim /etc/profile.d/my_env.sh
  ```

  添加如下内容

  ```shell
  #JAVA_HOME
  export JAVA_HOME=/opt/module/jdk1.8.0_311
  export PATH=$PATH:$JAVA_HOME/bin
  ```

  保存, source 一下/etc/profile 文件，让新的环境变量 PATH 生效即可

#### 编写集群分发脚本 xsync

> （1）需求：循环复制文件到所有节点的相同目录下

```shell
# 在/home/admin/bin 目录下创建 xsync 文件
# 在该文件中编写如下代码

#!/bin/bash
#1. 判断参数个数
if [ $# -lt 1 ]; then
	echo Not Enough Arguement!
	exit
fi
#2. 遍历集群所有机器
for host in hadoop102 hadoop103 hadoop104; do
	echo ==================== $host ====================
	#3. 遍历所有目录，挨个发送
	for file in $@; do
		#4. 判断文件是否存在
		if [ -e $file ]; then
			#5. 获取父目录
			pdir=$(
				cd -P $(dirname $file)
				pwd
			)
			#6. 获取当前文件的名称
			fname=$(basename $file)
			ssh $host "mkdir -p $pdir"
			rsync -av $pdir/$fname $host:$pdir
		else
			echo $file does not exists!
		fi
	done
done
```

> （2）修改脚本 xsync 具有执行权限
>
> ```shell
> [admin@hadoop102 bin]$ chmod +x xsync
> ```
>
> （3）测试脚本
>
> ```shell
> [admin@hadoop102 ~]$ xsync /home/admin/bin
> ```
>
> （4）将脚本复制到/bin 中，以便全局调用
>
> ```shell
> [admin@hadoop102 bin]$ sudo cp xsync /bin/
> ```
>
> （5）同步环境变量配置（root 所有者）
>
> ```shell
> [admin@hadoop102 ~]$ sudo ./bin/xsync /etc/profile.d/my_env.sh
> ```
>
> 注意：如果用了 sudo，那么 xsync 一定要给它的路径补全。
>
> （6）让环境变量生效
>
> ```shell
> [admin@hadoop102 bin]$ source /etc/profile
> ```



#### SSH 无密登录配置

需求：ssh 另一台电脑的ip地址

* 生成公钥和私钥

```shell
[root@hadoop102 .ssh]$ pwd
/root/.ssh
[root@hadoop102 .ssh]$ ssh-keygen -t rsa
```

然后敲（三个回车），就会生成两个文件 id_rsa（私钥）、id_rsa.pub（公钥）

* 将公钥拷贝到要免密登录的目标机器上

```shell
[root@hadoop102 .ssh]$ ssh-copy-id hadoop102
[root@hadoop102 .ssh]$ ssh-copy-id hadoop103
[root@hadoop102 .ssh]$ ssh-copy-id hadoop104
```

* 登录 hadoop103，104 ，在其上也执行一遍如上操作

```shell
# 登录 hadoop103
[root@hadoop103 .ssh]$ ssh-copy-id hadoop102
[root@hadoop103 .ssh]$ ssh-copy-id hadoop103
[root@hadoop103 .ssh]$ ssh-copy-id hadoop104
# 登录 hadoop104
[root@hadoop104 .ssh]$ ssh-copy-id hadoop102
[root@hadoop104 .ssh]$ ssh-copy-id hadoop103
[root@hadoop104 .ssh]$ ssh-copy-id hadoop104
```

* 切换成 admin 用户，然后重复一遍上面的操作即可【注意：不需要重新生成 id_rsa 了】

#### 同步 jdk 到 Hadoop103，Hadoop104

```shell
[admin@hadoop102 module]$ xsync jdk1.8.0_311/
# 同步 my_env.sh
[admin@hadoop102 module]$ xsync /etc/profile.d/my_env.sh
```



### Hadoop 安装

Hadoop 下载地址：https://archive.apache.org/dist/hadoop/common/hadoop-3.1.3/

1）用 XShell 文件传输工具将 hadoop-3.1.3.tar.gz 导入到 opt 目录下面的 software 文件夹下 面

2）进入到 Hadoop 安装包路径下 [atguigu@hadoop102 ~]$ cd /opt/software/

3）解压安装文件到/opt/module 下面

```shell
[admin@hadoop102 software]$ tar -zxvf hadoop-3.1.3.tar.gz -C /opt/module/
```

4）查看是否解压成功

```shell
[admin@hadoop102 software]$ ls /opt/module/ hadoop-3.1.3
```

 5）将 Hadoop 添加到环境变量

​	（1）获取 Hadoop 安装路径 `[admin@hadoop102 hadoop-3.1.3]$ pwd /opt/module/hadoop-3.1.3 `

​	（2）打开/etc/profile.d/my_env.sh 文件 `[admin@hadoop102 hadoop-3.1.3]$ sudo vim /etc/profile.d/my_env.sh`

```shell
#HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
```

### Hadoop Yarn集群配置

#### 配置 core-site.xml

核心配置文件 vim  $HADOOP_HOME/etc/hadoop/core-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
 <!-- 指定 NameNode 的地址 -->
 <property>
 <name>fs.defaultFS</name>
 <value>hdfs://hadoop102:8020</value>
 </property>
 <!-- 指定 hadoop 数据的存储目录 -->
 <property>
 <name>hadoop.tmp.dir</name>
 <value>/opt/module/hadoop-3.1.3/data</value>
 </property>
 <!-- 配置 HDFS 网页登录使用的静态用户为 admin，注意后期 hive 的 beeline 登录用户名也是这个 admin -->
 <property>
 <name>hadoop.http.staticuser.user</name>
 <value>admin</value>
 </property>
</configuration>
```

#### 配置 hdfs-site.xml

HDFS 配置文件 vim $HADOOP_HOME/etc/hadoop/hdfs-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<!-- nn web 端访问地址-->
<property>
 <name>dfs.namenode.http-address</name>
 <value>hadoop102:9870</value>
 </property>
<!-- 2nn web 端访问地址-->
 <property>
 <name>dfs.namenode.secondary.http-address</name>
 <value>hadoop104:9868</value>
 </property>
</configuration>
```

#### 配置 yarn-site.xml

YARN 配置文件 vim $HADOOP_HOME/etc/hadoop/yarn-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
 <!-- 指定 MR 走 shuffle -->
 <property>
 	<name>yarn.nodemanager.aux-services</name>
 	<value>mapreduce_shuffle</value>
 </property>
 <!-- 指定 ResourceManager 的地址-->
 <property>
 	<name>yarn.resourcemanager.hostname</name>
 	<value>hadoop103</value>
 </property>
 <!-- 环境变量的继承 -->
 <property>
 	<name>yarn.nodemanager.env-whitelist</name>
	<value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
 </property>
</configuration>
```

#### 配置 mapred-site.xml

MapReduce 配置文件 vim $HADOOP_HOME/etc/hadoop/mapred-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
<!-- 指定 MapReduce 程序运行在 Yarn 上 -->
 <property>
 	<name>mapreduce.framework.name</name>
 	<value>yarn</value>
 </property>
</configuration>
```

#### 配置 workers

vim $HADOOP_HOME/etc/hadoop/workers

在该文件中增加如下内容：

```shell
hadoop102
hadoop103
hadoop104
```

**注意：该文件中添加的内容结尾不允许有空格，文件中不允许有空行。**

#### 配置历史服务器

为了查看程序的历史运行情况，需要配置一下历史服务器。具体配置步骤如下：

##### 配置 mapred-site.xml

vim $HADOOP_HOME/etc/hadoop/mapred-site.xml

在该文件里面增加如下配置:

```xml
<!-- 历史服务器端地址 -->
<property>
 <name>mapreduce.jobhistory.address</name>
 <value>hadoop102:10020</value>
</property>
<!-- 历史服务器 web 端地址 -->
<property>
 <name>mapreduce.jobhistory.webapp.address</name>
 <value>hadoop102:19888</value>
</property>
```

查看 JobHistory: http://hadoop102:19888/jobhistory

#### 配置日志的聚集

日志聚集概念：应用运行完成以后，将程序运行日志信息上传到 HDFS 系统上。

注意：开启日志聚集功能，需要重新启动 NodeManager 、ResourceManager 和 HistoryServer。此处建议直接重启 hdfs ,yarn

##### 配置 yarn-site.xml

vim $HADOOP_HOME/etc/hadoop/yarn-site.xml

在该文件里面增加如下配置。

```xml
<!-- 开启日志聚集功能 -->
<property>
 <name>yarn.log-aggregation-enable</name>
 <value>true</value>
</property>
<!-- 设置日志聚集服务器地址 -->
<property>
 <name>yarn.log.server.url</name>
 <value>http://hadoop102:19888/jobhistory/logs</value>
</property>
<!-- 设置日志保留时间为 7 天 -->
<property>
 <name>yarn.log-aggregation.retain-seconds</name>
 <value>604800</value>
</property>
```

#### 集群启动/停止方式

（1）各个模块分开启动/停止（配置 ssh 是前提）常用

`start-dfs.sh/stop-dfs.sh`

（2）整体启动/停止 YARN

`start-yarn.sh/stop-yarn.sh`

（3）各个服务组件逐一启动/停止

* 分别启动/停止 HDFS 组件 `hdfs --daemon start/stop namenode/datanode/secondarynamenode`
* 启动/停止 YARN `yarn --daemon start/stop resourcemanager/nodemanager`

##### Hadoop 集群启停脚本（包含 HDFS，Yarn，Historyserver）：myhadoop.sh

cd /home/admin/bin

vim myhadoop.sh

➢ 输入如下内容

```shell
#!/bin/bash
if [ $# -lt 1 ]; then
	echo "No Args Input..."
	exit
fi
case $1 in
"start")
	echo " =================== 启动 hadoop 集群 ==================="
	echo " --------------- 启动 hdfs ---------------"
	ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/start-dfs.sh"
	echo " --------------- 启动 yarn ---------------"
	ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/start-yarn.sh"
	echo " --------------- 启动 historyserver ---------------"
	ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon start
historyserver"
	;;
"stop")
	echo " =================== 关闭 hadoop 集群 ==================="
	echo " --------------- 关闭 historyserver ---------------"
	ssh hadoop102 "/opt/module/hadoop-3.1.3/bin/mapred --daemon stop
historyserver"
	echo " --------------- 关闭 yarn ---------------"
	ssh hadoop103 "/opt/module/hadoop-3.1.3/sbin/stop-yarn.sh"
	echo " --------------- 关闭 hdfs ---------------"
	ssh hadoop102 "/opt/module/hadoop-3.1.3/sbin/stop-dfs.sh"
	;;
*)
	echo "Input Args Error..."
	;;
esac
```

注意hadoop版本，路径确认一致

赋予脚本执行权限 `chmod +x myhadoop.sh`

#### 查看三台服务器 Java 进程脚本：jpsall

cd /home/admin/bin

vim jpsall

➢ 输入如下内容

```shell
#!/bin/bash
for host in hadoop102 hadoop103 hadoop104
do
 echo =============== $host ===============
 ssh $host jps
done
```

赋予脚本执行权限 `chmod +x jpsall`

#### 分发内容到其它主机

前面已经将 jdk 分发了，此处只需要分发 hadoop my_env.sh 即可

```shell
# 分发/home/admin/bin 目录，保证自定义脚本在三台机器上都可以使用
xsync /home/admin/bin/
# hadoop
xsync /opt/module/hadoop-3.1.3/
# my_env.sh
xsync /etc/profile.d/my_env.sh
```

#### 启动集群测试

```shell
# 启动
myhadoop.sh start
# jps
jpsall
```



## Hive 搭建

hive 的元素据选择存储在mysql中，因此需要安装 mysql

#### hive 包准备

apache-hive-3.0.0-bin.tar.gz (从硬盘中获取该文件，上传到 hadoop102/opt/software/目录下)

解压到 /opt/module/ 目录下，重命名为 hive-3.0.0

#### MySQL安装

参考文章：https://hello-github-ui.github.io/posts/8567/

#### 配置hive文件

修改/opt/module/hive-3.0.0/conf/hive-env.sh

`cp hive-env.sh.template hive-env.sh`

增加内容如下：

```shell
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export HIVE_CONF_DIR=/opt/module/hive-3.0.0/conf
export HIVE_AUX_JARS_PATH=/opt/module/hive-3.0.0/lib
```

#### 配置hive-site.xml文件

直接新建 hive-site.xml 文件

vim hive-site.xml

```xml
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
        <!--不使用本地数据库 -->
        <property>
                <name>hive.metastore.local</name>
                <value>false</value>
        </property>

        <!--Default数据仓库原始位置是在hdfs上：/user/hive/warehouse路径下-->
        <property>
                <name>hive.metastore.warehouse.dir</name>
                <value>/hive/warehouse</value>
                <!--先在HDFS创建目录-->
                <description>location of default database for the warehouse</description>
        </property>

        <!--Hive用来存储不同阶段的MapReduce的执行计划的目录，同时也存储中间输出结果-->
        <property>
                <name>hive.exec.scratchdir</name>
                <value>/tmp/hive</value>
                <!--先在HDFS创建目录-->
        </property>

        <property>
                <name>hive.scratch.dir.permission</name>
                <value>777</value>
                <description>The permission for the user specific scratch directories that get created.</description>
        </property>

        <!--当Hive运行在本地模式时配置-->
        <property>
                <name>hive.exec.local.scratchdir</name>
                <value>/opt/module/hive-3.0.0/tmp/hive/root</value>
                <!--先在本地创建目录-->
                <description>Local scratch space for Hive jobs</description>
        </property>

        <!--远程资源下载的临时目录-->
        <property>
                <name>hive.downloaded.resources.dir</name>
                <value>/opt/module/hive-3.0.0/tmp/resources</value>
                <!--先在本地创建目录-->
                <description>Temporary local directory for added resources in the remote file system.</description>
        </property>

        <!--配置Metastore到MySql-->
        <property>
                <name>javax.jdo.option.ConnectionURL</name>
                <value>jdbc:mysql://localhost:3306/metastore?createDatabaseIfNotExist=true&amp;useSSL=false&amp;allowPublicKeyRetrieval=true</value>
                <!--XML需要转义，&转义为&amp;-->
                <description>JDBC connect string for a JDBC metastore</description>
        </property>

        <property>
                <name>javax.jdo.option.ConnectionDriverName</name>
                <value>com.mysql.cj.jdbc.Driver</value>
                <description>Driver class name for a JDBC metastore</description>
        </property>

        <property>
                <name>javax.jdo.option.ConnectionUserName</name>
                <value>root</value>
                <description>username to use against metastore database</description>
        </property>

        <property>
                <name>javax.jdo.option.ConnectionPassword</name>
                <value>123456</value>
                <description>password to use against metastore database</description>
        </property>

        <!--查询后显示当前数据库，以及查询表的头信息配置-->
        <property>
                <name>hive.cli.print.header</name>
                <value>true</value>
        </property>

        <property>
                <name>hive.cli.print.current.db</name>
                <value>true</value>
        </property>

        <property>
                <name>hive.metastore.schema.verification</name>
                <value>false</value>
        </property>

        <!--metastore的端口-->
        <property>
                <name>hive.metastore.uris</name>
                <value>thrift://hadoop102:9083</value>
        </property>

</configuration>
```

#### hive连接MySQL驱动连接包

拷贝 mysql-connector-java-8.0.26.jar 包 ，将其放到 /opt/module/hive-3.0.0/lib 目录下。

硬盘上mysql目录下应该有这个 connector 包

#### 配置 hive 日志

```shell
mv hive-log4j2.properties.template hive-log4j2.properties
```

在 /opt/module/hive-3.0.0/ 新建目录 log，用于存放 hive 日志

修改 hive-log4j2.properties 配置：

```shell
# 找到改行内容，将其修改为如下
property.hive.log.dir = /opt/module/hive-3.0.0/log
```

#### 新建hive仓库目录

```shell
hdfs dfs -mkdir -p /hive/warehouse
hdfs dfs -mkdir -p /tmp/hive
```

#### 配置环境变量

```shell
# vim /etc/profile.d/my_env.sh
#JAVA_HOME
export JAVA_HOME=/opt/module/jdk1.8.0_311
export PATH=$PATH:$JAVA_HOME/bin

#HADOOP_HOME
export HADOOP_HOME=/opt/module/hadoop-3.1.3
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

#HIVE_HOME
export HIVE_HOME=/opt/module/hive-3.0.0
export PATH=$PATH:$HIVE_HOME/bin

export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS=-Djava.library.path=$HADOOP_HOME/lib
```

#### 元数据库初始化

进入目录 /opt/module/hive-3.0.0/bin

```shell
schematool -dbType mysql -initSchema
```

#### 启动hive

> 注意：启动hive前，一定要先启动 metastore 或者 hiveserver2
> Spark Thrift Server 是 Spark 社区基于 HiveServer2 实现的一个 Thrift 服务。旨在无缝兼容HiveServer2。因为 Spark Thrift Server 的接口和协议都和 HiveServer2 完全一致，因此我们部
署好 Spark Thrift Server 后，可以直接使用 hive 的 beeline 访问 Spark Thrift Server 执行相关语句。Spark Thrift Server 的目的也只是取代 HiveServer2，因此它依旧可以和 Hive Metastore
进行交互，获取到 hive 的元数据。

此处通过 Thrift 服务来启动 客户端配置：`sh /opt/module/spark-yarn/sbin/start-thriftserver.sh`
使用 beeline 连接 Thrift Server: `bin/beeline -u jdbc:hive2://hadoop102:10000 -n admin`

> 当然你也可以通过启动 metasotre，进入hive的bin目录 : `./hive --service metastore &`，然后再启动hive: `hive`

## Spark 搭建

spark-3.0.0-bin-hadoop3.2.tgz 准备，从硬盘上获取

上传到 hadoop102  /opt/software/ 上，解压缩到 /opt/module/ 下

重命名为 mv spark-3.0.0-bin-hadoop3.2 spark-yarn

编辑文件 vim /opt/module/spark-yarn/conf/spark-env.sh 文末增加如下内容

```shell
export JAVA_HOME=/opt/module/jdk1.8.0_311
YARN_CONF_DIR=/opt/module/hadoop-3.1.3/etc/hadoop

export SPARK_HISTORY_OPTS="
 -Dspark.history.ui.port=18080
 -Dspark.history.fs.logDirectory=hdfs://hadoop102:8020/directory
 -Dspark.history.retainedApplications=30
```

#### Spark连接外置hive

通常来说如果你下载的是二进制版本的Spark，它应该已经在编译时添加了hive支持

若要把 Spark SQL连接到一个部署好的hive上，你必须把 hive-site.xml 复制到 Spark的配置文件目录中，即使没有部署好 hive，Spark SQL 也可以运行。因为实际中这种方式比较少，所以我们这里主要来讲 连接 外置hive

如果想连接外部已经部署好的 Hive，需要通过以下几个步骤：

➢ Spark 要接管 Hive 需要把 `/opt/module/hive-3.0.0/conf/hive-site.xml` 拷贝到 `/opt/module/spark-yarn/conf/` 目录下

➢ 把 Mysql 的驱动 copy 到 `/opt/module/spark-yarn/jars/`目录下

➢ 如果访问不到 hdfs，则需要把 `/opt/module/hadoop-3.1.3/etc/hadoop/core-site.xml` 和 `/opt/module/hadoop-3.1.3/etc/hadoop/hdfs-site.xml` 拷贝到 conf/目录下

➢ 重启 spark-shell





